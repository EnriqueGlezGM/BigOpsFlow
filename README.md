# Food Delivery â€“ Streaming Predictions (Kafka â†’ Spark â†’ Mongo/Elastic â†’ Flask UI)

Pipeline de predicciÃ³n en tiempo real con:
- **Kafka** (peticiones/respuestas),
- **Spark Structured Streaming** (feature engineering + modelo),
- **MongoDB** (trazabilidad),
- **Elasticsearch + Kibana** (bÃºsqueda/observabilidad),
- **Flask** (formulario web de entrada y polling de resultados),
- **Jupyter/Notebook** (job de streaming sencillo de ejecutar).

---

## ğŸ§± Requisitos

- Docker y Docker Compose
- Puertos libres:
  - Jupyter/Agile: `8888` (si lo expones)
  - Flask: `5050`
  - Kafka: `9092`
  - Mongo Express: `8081`
  - Elasticsearch: `9200`
  - Kibana: `5601`
  - Spark Master UI: `8080`

---

## ğŸ“¦ Servicios (resumen)

- **agile**: entorno con Spark + Jupyter (para el job de streaming/notebooks).
- **predict_api**: Flask con formulario y endpoints (`/mydata/predict` + polling).
- **kafka**: broker Kafka.
- **mongo** + **mongo-express**: almacenamiento y UI simple.
- **elastic** + **kibana**: bÃºsqueda/visualizaciÃ³n.
- **(opcional)** script de bootstrap para crear **pipeline**, **index template**, **Ã­ndice base**, **data view** y **modo oscuro** en Kibana.

---

## ğŸš€ Arranque rÃ¡pido

```bash
docker compose up -d --build
```

Espera ~30â€“60s a que todo quede â€œhealthyâ€.

---

## âœ… Comprobaciones rÃ¡pidas

- **Spark Master UI**: http://localhost:8080  
- **Mongo Express**: http://localhost:8081  
  - DB `agile_data_science`
  - Colecciones: `mydata_prediction_response`, `mydata_prediction_errors`
- **Kibana**: http://localhost:5601  
  - Data View: `mydata_prediction_response*`
  - Campo temporal: `@ingest_ts`
- **Flask UI**: http://localhost:5050  

---

## ğŸ“¡ TÃ³picos de Kafka

- **Entrada**: `mydata_prediction_request`
- **Salida**: `mydata_prediction_response`

---

## ğŸ”® Modelo

El modelo se carga desde `./models/pipeline_model.bin` (dentro de **agile**).  
AsegÃºrate de que el path existe en el contenedor.

---

## ğŸ§ª Probar de extremo a extremo

### 1) Lanza el job de streaming (Notebook)

```bash
docker exec -it agile bash
```

Luego abre Jupyter y ejecuta **Run All** en el notebook `MY_Make predictions.ipynb`  
o bien usa el script `.py` equivalente.

El job debe mostrar logs tipo:

```
ğŸ”§ Creando SparkSession...
âœ… SparkSession creada
ğŸ”§ Cargando PipelineModel...
âœ… Modelo cargado
ğŸ”Œ Conectando a Kafka (topic: mydata_prediction_request)...
ğŸš€ Iniciando streaming (Mongo + Kafka + Elasticsearch)...
â³ Esperando microbatches...
```

### 2) Abre la UI Flask

http://localhost:5050  
- Rellena el formulario y envÃ­alo.  
- Se devuelve un `UUID` y empieza el **polling**.  
- Cuando Spark produce la predicciÃ³n, aparece en pantalla y se guarda en **Mongo**, **Kafka** y **Elasticsearch**.

### 3) Revisa en Kibana

- Abre **Discover**, Data View `mydata_prediction_response*`.
- Ajusta el rango de fechas â†’ â€œNowâ€.
- VerÃ¡s documentos con `UUID`, `order_id`, `prediction`, `@ingest_ts`, etc.

---

## ğŸ§° Endpoints Ãºtiles (Flask)

- `POST /mydata/predict` â†’ envÃ­a un pedido, retorna `{"id": "...", "status": "submitted"}`  
- `GET /mydata/predict/response/<UUID>` â†’ polling hasta tener predicciÃ³n

---

## ğŸ§ª Ejemplo de payload

```json
{
  "UUID": "generado-por-el-frontend",
  "order_id": 1,
  "customer_id": "cust_123",
  "restaurant_id": "rest_456",
  "order_date_and_time": "2025-08-06T13:00:00",
  "delivery_date_and_time": "2025-08-06T13:45:00",
  "order_value": 1000,
  "delivery_fee": 100,
  "payment_method": "credit_card",
  "discounts_and_offers": "10% off",
  "commission_fee": 100,
  "payment_processing_fee": 20,
  "refunds/chargebacks": 0
}
```

---

## ğŸ§© Bootstrap de Elasticsearch/Kibana (automÃ¡tico)

- Ingest Pipeline (quita `_id`, aÃ±ade `@ingest_ts`, normaliza campos).
- Index Template e Ã­ndice base `mydata_prediction_response`.
- Data View en Kibana (`mydata_prediction_response*`).
- Ajustes avanzados (modo oscuro, etc.).

---

## ğŸ§¹ Parar y limpiar

```bash
docker compose down
docker compose down -v   # âš ï¸ borra volÃºmenes (Mongo/Elastic/Kibana)
```

---

## ğŸ©º Troubleshooting rÃ¡pido

- **No aparecen jobs en Spark UI** â†’ revisa SparkSession en el notebook.
- **No hay datos en Kibana** â†’ revisa Mongo (`mydata_prediction_response`), amplÃ­a rango de fechas, verifica logs.
- **Errores en Elasticsearch** â†’ pueden deberse a formatos de fecha; revisa logs del streaming.
- **Kafka vacÃ­o** â†’ confirma que el formulario hace POST y que el job estÃ¡ leyendo `mydata_prediction_request`.

---

## ğŸ—ï¸ Estructura del repo

```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.predict_api
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init-elastic-kibana.sh
â”œâ”€â”€ Food_delivery/
â”‚   â””â”€â”€ Deploying_Predictive_Systems/
â”‚       â”œâ”€â”€ MY_Make predictions.ipynb
â”‚       â”œâ”€â”€ web/
â”‚       â”‚   â”œâ”€â”€ MY_flask_api.py
â”‚       â”‚   â””â”€â”€ my_form.html
â”‚       â””â”€â”€ models/
â”‚           â””â”€â”€ pipeline_model.bin
â””â”€â”€ README.md
```

---

## ğŸ§­ Â¿Por quÃ© usar notebook en vez de spark-submit?

- **Transparencia**: cada paso es visible y reproducible.  
- **IteraciÃ³n rÃ¡pida**: cambiar lÃ³gica sin recompilar ni reiniciar contenedores.  
- **Paridad**: el cÃ³digo del notebook es 1:1 con la versiÃ³n `.py`.  
- **Simplicidad**: para demo/PoC, menos fricciÃ³n.