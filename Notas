Dentro de agile:
    $SPARK_HOME/sbin/start-master.sh

    $SPARK_HOME/sbin/start-worker.sh spark://agile:7077

    SPARK_WORKER_PORT=7078 \
    SPARK_WORKER_WEBUI_PORT=8081 \
    $SPARK_HOME/bin/spark-class org.apache.spark.deploy.worker.Worker spark://agile:7077 &


En notebook:
    from pyspark.sql import SparkSession
    from pyspark import SparkContext


    if SparkContext._active_spark_context:
        SparkContext._active_spark_context.stop()
        
    APP_NAME = "Introducing PySpark"

    spark = (
        SparkSession.builder.appName(APP_NAME)
        .master("spark://agile:7077")  # << ðŸš€ AquÃ­ defines que usas el cluster Spark interno
        .config("spark.driver.bindAddress", "0.0.0.0")  # << âœ… Necesario si ejecutas desde Jupyter
        # Load support for MongoDB and Elasticsearch
        .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.elasticsearch:elasticsearch-spark-30_2.12:7.14.2")
        # Add Configuration for MongoDB
        .config("spark.mongodb.input.uri", "mongodb://mongo:27017/test.coll")
        .config("spark.mongodb.output.uri", "mongodb://mongo:27017/test.coll")
        .getOrCreate()
    )

    sc = spark.sparkContext
    sc.setLogLevel("ERROR")

    print("\nâœ… PySpark inicializado usando Spark Master en spark://localhost:7077")