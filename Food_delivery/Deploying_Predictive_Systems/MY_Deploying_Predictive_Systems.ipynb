{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Predictive Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T16:27:31.921051Z",
     "start_time": "2025-07-04T16:27:21.971649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/30 15:56:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Cerrar SparkContext anterior si está activo\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "# SparkSession apuntando al cluster\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DeployMyModel\")\n",
    "    .master(\"spark://agile:7077\")  # Asegúrate de que 'agile' es el nombre correcto del master en tu Docker Compose\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n",
      "spark://agile:7077\n",
      "Aplicación: DeployMyModel\n",
      "Set(agile:37701)\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)\n",
    "print(spark.sparkContext.master)\n",
    "print(\"Aplicación:\", spark.sparkContext.appName)\n",
    "print(spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/home/jovyan/.cache/kagglehub/datasets/romanniki/food-delivery-cost-and-profitability/versions/1/food_orders_new_delhi (1).csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp \"/home/jovyan/.cache/kagglehub/datasets/romanniki/food-delivery-cost-and-profitability/versions/1/food_orders_new_delhi (1).csv\" ./data/food_orders.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+\n",
      "|Order ID|Customer ID|Restaurant ID|Order Date and Time|Delivery Date and Time|Order Value|Delivery Fee|  Payment Method|Discounts and Offers|Commission Fee|Payment Processing Fee|Refunds/Chargebacks|\n",
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+\n",
      "|       1|      C8270|        R2924|2024-02-01 01:11:52|   2024-02-01 02:39:52|       1914|           0|     Credit Card|           5% on App|           150|                    47|                  0|\n",
      "|       2|      C1860|        R2054|2024-02-02 22:11:04|   2024-02-02 22:46:04|        986|          40|  Digital Wallet|                 10%|           198|                    23|                  0|\n",
      "|       3|      C6390|        R2870|2024-01-31 05:54:35|   2024-01-31 06:52:35|        937|          30|Cash on Delivery|        15% New User|           195|                    45|                  0|\n",
      "|       4|      C6191|        R2642|2024-01-16 22:52:49|   2024-01-16 23:38:49|       1463|          50|Cash on Delivery|                None|           146|                    27|                  0|\n",
      "|       5|      C6734|        R2799|2024-01-29 01:19:30|   2024-01-29 02:48:30|       1992|          30|Cash on Delivery|        50 off Promo|           130|                    50|                  0|\n",
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = \"./data/food_orders.csv\"\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, hour, when, col\n",
    "\n",
    "# Añadir columnas calculadas en Spark\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"Order Date and Time\"))  # 1=Sunday, ..., 7=Saturday\n",
    "df = df.withColumn(\"hour_of_day\", hour(\"Order Date and Time\"))\n",
    "\n",
    "# Es fin de semana (sábado=7 o domingo=1)\n",
    "df = df.withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# Es hora punta (13-15 o 20-22)\n",
    "df = df.withColumn(\n",
    "    \"es_hora_punta\",\n",
    "    when((col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+-----------+-----------+----------------+-------------+\n",
      "|Order ID|Customer ID|Restaurant ID|Order Date and Time|Delivery Date and Time|Order Value|Delivery Fee|  Payment Method|Discounts and Offers|Commission Fee|Payment Processing Fee|Refunds/Chargebacks|day_of_week|hour_of_day|es_fin_de_semana|es_hora_punta|\n",
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+-----------+-----------+----------------+-------------+\n",
      "|       1|      C8270|        R2924|2024-02-01 01:11:52|   2024-02-01 02:39:52|       1914|           0|     Credit Card|           5% on App|           150|                    47|                  0|          5|          1|               0|            0|\n",
      "|       2|      C1860|        R2054|2024-02-02 22:11:04|   2024-02-02 22:46:04|        986|          40|  Digital Wallet|                 10%|           198|                    23|                  0|          6|         22|               0|            1|\n",
      "|       3|      C6390|        R2870|2024-01-31 05:54:35|   2024-01-31 06:52:35|        937|          30|Cash on Delivery|        15% New User|           195|                    45|                  0|          4|          5|               0|            0|\n",
      "|       4|      C6191|        R2642|2024-01-16 22:52:49|   2024-01-16 23:38:49|       1463|          50|Cash on Delivery|                None|           146|                    27|                  0|          3|         22|               0|            1|\n",
      "|       5|      C6734|        R2799|2024-01-29 01:19:30|   2024-01-29 02:48:30|       1992|          30|Cash on Delivery|        50 off Promo|           130|                    50|                  0|          2|          1|               0|            0|\n",
      "+--------+-----------+-------------+-------------------+----------------------+-----------+------------+----------------+--------------------+--------------+----------------------+-------------------+-----------+-----------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Order ID\", \"order_id\") \\\n",
    "       .withColumnRenamed(\"Customer ID\", \"customer_id\") \\\n",
    "       .withColumnRenamed(\"Restaurant ID\", \"restaurant_id\") \\\n",
    "       .withColumnRenamed(\"Order Date and Time\", \"order_date_and_time\") \\\n",
    "       .withColumnRenamed(\"Delivery Date and Time\", \"delivery_date_and_time\") \\\n",
    "       .withColumnRenamed(\"Order Value\", \"order_value\") \\\n",
    "       .withColumnRenamed(\"Delivery Fee\", \"delivery_fee\") \\\n",
    "       .withColumnRenamed(\"Payment Method\", \"payment_method\") \\\n",
    "       .withColumnRenamed(\"Discounts and Offers\", \"discounts_and_offers\") \\\n",
    "       .withColumnRenamed(\"Commission Fee\", \"commission_fee\") \\\n",
    "       .withColumnRenamed(\"Payment Processing Fee\", \"payment_processing_fee\") \\\n",
    "       .withColumnRenamed(\"Refunds/Chargebacks\", \"refunds/chargebacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, hour, when, col\n",
    "\n",
    "# Día de la semana y hora\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "df = df.withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "\n",
    "# Fin de semana\n",
    "df = df.withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# Hora punta\n",
    "df = df.withColumn(\"es_hora_punta\", when(\n",
    "    (col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1\n",
    ").otherwise(0))\n",
    "\n",
    "# Tiene descuento\n",
    "df = df.withColumn(\"has_discount\", when(\n",
    "    col(\"discounts_and_offers\").isNotNull() & (col(\"discounts_and_offers\") != \"None\"), 1\n",
    ").otherwise(0))\n",
    "\n",
    "# Valor del descuento\n",
    "df = df.withColumn(\"discount_value\", col(\"order_value\") * col(\"has_discount\").cast(\"int\") * 0.1)\n",
    "\n",
    "df = df.withColumn(\"refunded\", (col(\"refunds/chargebacks\") > 0).cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       prediction|\n",
      "+-----------------+\n",
      "|79.84346673549138|\n",
      "| 61.0080084099073|\n",
      "|81.72973689000804|\n",
      "|60.31104433039575|\n",
      "|82.14852090974995|\n",
      "|62.91276068214107|\n",
      "|66.69234628045997|\n",
      "|86.82624548541419|\n",
      "|72.10926636991185|\n",
      "|76.91737711333077|\n",
      "+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "predicciones = modelo.transform(df)\n",
    "\n",
    "predicciones.select(\"prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
