{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f854b8",
   "metadata": {},
   "source": [
    "Script en celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25191f",
   "metadata": {},
   "source": [
    "En dos terminales ejecutar:\n",
    "\n",
    "```bash\n",
    "docker exec -it kafka bash\n",
    "```\n",
    "y luego en uno esto:\n",
    "\n",
    "```bash\n",
    " ./opt/kafka/bin/kafka-console-consumer.sh \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic mydata_prediction_response\n",
    "```\n",
    "\n",
    "y en otro:\n",
    "```bash\n",
    "/opt/kafka/bin/kafka-console-producer.sh   --broker-list kafka:9092   --topic mydata_prediction_request   --property \"parse.key=false\"   --property \"key.separator=:\"   --property \"value.serializer=org.apache.kafka.common.serialization.StringSerializer\"\n",
    "```\n",
    "y de ejemplo en el producer:\n",
    "```bash\n",
    "{\"order_id\": 1, \"customer_id\": \"cust_123\", \"restaurant_id\": \"rest_456\", \"order_date_and_time\": \"2025-08-06T13:00:00\", \"delivery_date_and_time\": \"2025-08-06T13:45:00\", \"order_value\": 1000, \"delivery_fee\": 100, \"payment_method\": \"credit_card\", \"discounts_and_offers\": \"10% off\", \"commission_fee\": 100, \"payment_processing_fee\": 20, \"refunds/chargebacks\": 0}\n",
    "```\n",
    "\n",
    "Pudes abrir mongo express y ver el resultado http://localhost:8081/db/agile_data_science/mydata_prediction_response\n",
    "\n",
    "Para ejecutar el form:\n",
    "```bash\n",
    "docker exec -it agile bash\n",
    "``` \n",
    "y dentro de agile:\n",
    "```bash\n",
    "python /home/jovyan/Food_delivery/Deploying_Predictive_Systems/web/MY_flask_api.py\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e5d23a-8c70-4632-b5a6-a990986510b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creando SparkSession...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4bd93764-a28e-4247-927f-6ed91910a4f2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1082ms :: artifacts dl 30ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4bd93764-a28e-4247-927f-6ed91910a4f2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/22ms)\n",
      "25/08/14 17:03:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession creada\n",
      "üîß Cargando PipelineModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo cargado\n",
      "üîå Conectando a Kafka (topic: mydata_prediction_request)...\n",
      "üöÄ Iniciando streaming (Mongo + Kafka + Elasticsearch)...\n",
      "‚è≥ Esperando microbatches...\n",
      "üß± Microbatch 0: 0 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß± Microbatch 1: 1 docs\n",
      "üì¶ ES bulk resp: {\"errors\":true,\"took\":0,\"ingest_took\":0,\"items\":[{\"index\":{\"_index\":\"mydata_prediction_response\",\"_id\":\"E-aKqZgBZOyNk9YBcnsU\",\"status\":400,\"error\":{\"type\":\"document_parsing_exception\",\"reason\":\"[1:285] failed to parse field [@ingest_ts] of type [date] in document with id 'E-aKqZgBZOyNk9YBcnsU'. Preview of field's value: '2025-08-14 17:04:32.973000'\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"failed to parse date field [2025-08-14 17:04:32.973000] with format [strict_date_optional ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.                                     \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=64>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 292, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 1195, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1365, in __getattr__\n",
      "    def __getattr__(self, name):\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 292, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 1195, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o22.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o552.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_356/4205630230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚è≥ Esperando microbatches...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o552.awaitTermination"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CELDA √öNICA: Streaming Kafka -> Enriquecido -> Predicci√≥n -> Mongo + Kafka + Elasticsearch\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, LongType\n",
    "from pyspark.sql.functions import from_json, col, hour, dayofweek, when, current_timestamp\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "print(\"üîß Creando SparkSession...\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"mydata-streaming-predict\")\n",
    "    .master(\"spark://agile:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"‚úÖ SparkSession creada\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Cargar modelo\n",
    "# -------------------------\n",
    "print(\"üîß Cargando PipelineModel...\")\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "print(\"‚úÖ Modelo cargado\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Esquema de entrada (incluye UUID)\n",
    "# -------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"UUID\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"order_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"delivery_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"delivery_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"discounts_and_offers\", StringType(), True),\n",
    "    StructField(\"commission_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_processing_fee\", DoubleType(), True),\n",
    "    StructField(\"refunds/chargebacks\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# 3) Lectura desde Kafka\n",
    "# -------------------------\n",
    "print(\"üîå Conectando a Kafka (topic: mydata_prediction_request)...\")\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"mydata_prediction_request\")\n",
    "    .option(\"startingOffsets\", \"latest\")   # cambia a \"earliest\" si quieres re-consumir desde el inicio\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_df = (\n",
    "    raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json_data\")\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Enriquecido EXACTO como en el entrenamiento\n",
    "# -------------------------\n",
    "df_enriched = (\n",
    "    json_df\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "    .withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"es_hora_punta\", when(\n",
    "        (col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1\n",
    "    ).otherwise(0))\n",
    "    .withColumn(\"has_discount\", when(col(\"discounts_and_offers\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"discount_value\", when(col(\"has_discount\") == 1, col(\"order_value\") * 0.1).otherwise(0.0))\n",
    "    .withColumn(\"refunded\", when(col(\"refunds/chargebacks\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Selecci√≥n de columnas:\n",
    "#    - las que el pipeline necesita\n",
    "#    - + columnas de CONTEXTO que queremos conservar (pasan a trav√©s del modelo)\n",
    "# -------------------------\n",
    "required_numeric = [\n",
    "    \"order_value\", \"delivery_fee\", \"commission_fee\", \"payment_processing_fee\",\n",
    "    \"refunds/chargebacks\", \"discount_value\", \"has_discount\", \"refunded\",\n",
    "    \"day_of_week\", \"hour_of_day\", \"es_fin_de_semana\", \"es_hora_punta\"\n",
    "]\n",
    "required_categorical = [\"payment_method\", \"discounts_and_offers\"]\n",
    "context_cols = [\"UUID\", \"order_id\", \"order_date_and_time\", \"payment_method\", \"discounts_and_offers\"]\n",
    "features_df = df_enriched.select(*(required_numeric + required_categorical + context_cols))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Predicci√≥n\n",
    "# -------------------------\n",
    "predicciones = modelo.transform(features_df)\n",
    "\n",
    "# Documento final (sin joins)\n",
    "resultado_enriquecido = (\n",
    "    predicciones\n",
    "    .select(\n",
    "        \"UUID\",\n",
    "        \"order_id\",\n",
    "        \"prediction\",\n",
    "        \"order_date_and_time\",\n",
    "        \"payment_method\",\n",
    "        \"discounts_and_offers\",\n",
    "        \"day_of_week\",\n",
    "        \"hour_of_day\",\n",
    "        \"es_fin_de_semana\",\n",
    "        \"es_hora_punta\"\n",
    "    )\n",
    "    .withColumn(\"@ingest_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7) foreachBatch: Mongo + Kafka + Elasticsearch (Bulk)\n",
    "# -------------------------\n",
    "def write_to_mongo_kafka_es(batch_df, epoch_id):\n",
    "    import pymongo, json, urllib.request, urllib.error\n",
    "    from uuid import uuid4\n",
    "\n",
    "    client = None\n",
    "    try:\n",
    "        # Colectar microbatch (en dicts normales)\n",
    "        rows = [r.asDict() for r in batch_df.collect()]\n",
    "        print(f\"üß± Microbatch {epoch_id}: {len(rows)} docs\")\n",
    "\n",
    "        if not rows:\n",
    "            return\n",
    "\n",
    "        # --- Prepara versi√≥n SANEADA para ES (NUNCA con _id)\n",
    "        es_rows = []\n",
    "        for d in rows:\n",
    "            d_es = dict(d)        # copia\n",
    "            d_es.pop(\"_id\", None) # por si viniera de otra etapa\n",
    "            es_rows.append(d_es)\n",
    "\n",
    "        # --- Construye y env√≠a BULK NDJSON a ES ANTES de tocar Mongo\n",
    "        ndjson_lines = []\n",
    "        for d in es_rows:\n",
    "            es_id = d.get(\"UUID\") or str(uuid4())  # idempotencia\n",
    "            ndjson_lines.append(json.dumps({\"index\": {\"_index\": \"mydata_prediction_response\", \"_id\": es_id}}))\n",
    "            ndjson_lines.append(json.dumps(d, default=str))\n",
    "        payload = (\"\\n\".join(ndjson_lines) + \"\\n\").encode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            req = urllib.request.Request(\n",
    "                \"http://elastic:9200/_bulk\",\n",
    "                data=payload,\n",
    "                headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "                method=\"POST\",\n",
    "            )\n",
    "            with urllib.request.urlopen(req, timeout=15) as resp:\n",
    "                body = resp.read().decode(\"utf-8\", errors=\"replace\")\n",
    "                print(\"üì¶ ES bulk resp:\", body[:500], \"...\")\n",
    "        except urllib.error.HTTPError as he:\n",
    "            body = he.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            print(f\"‚ùå ES HTTPError {he.code}: {body[:800]}\")\n",
    "\n",
    "        # --- Kafka (respuesta): usa el batch_df tal cual\n",
    "        (batch_df\n",
    "         .selectExpr(\"UUID as key\", \"to_json(struct(*)) as value\")\n",
    "         .write\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "         .option(\"topic\", \"mydata_prediction_response\")\n",
    "         .save())\n",
    "\n",
    "        # --- Mongo: inserta COPIAS para que PyMongo pueda inyectar _id sin contaminar nada\n",
    "        client = pymongo.MongoClient(\"mongo\")\n",
    "        db = client[\"agile_data_science\"]\n",
    "        out = db[\"mydata_prediction_response\"]\n",
    "        out.create_index(\"UUID\", unique=False)\n",
    "\n",
    "        rows_for_mongo = [dict(d) for d in es_rows]  # copia independiente\n",
    "        out.insert_many(rows_for_mongo)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en foreachBatch:\", e)\n",
    "        try:\n",
    "            if client is None:\n",
    "                client = pymongo.MongoClient(\"mongo\")\n",
    "            db = client[\"agile_data_science\"]\n",
    "            db[\"mydata_prediction_errors\"].insert_one({\n",
    "                \"epoch_id\": int(epoch_id),\n",
    "                \"error\": str(e),\n",
    "                \"note\": \"Fallo en foreachBatch\",\n",
    "            })\n",
    "        except Exception as e2:\n",
    "            print(\"‚ùå Error registrando error en Mongo:\", e2)\n",
    "    finally:\n",
    "        try:\n",
    "            client and client.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# -------------------------\n",
    "# 8) Lanzar el stream\n",
    "# -------------------------\n",
    "print(\"üöÄ Iniciando streaming (Mongo + Kafka + Elasticsearch)...\")\n",
    "query = (resultado_enriquecido.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", \"/tmp/checkpoints-foreachbatch-es-v6\")  # cambia si reinicias\n",
    "         .foreachBatch(write_to_mongo_kafka_es)\n",
    "         .start())\n",
    "\n",
    "print(\"‚è≥ Esperando microbatches...\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
