{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f854b8",
   "metadata": {},
   "source": [
    "Script en celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25191f",
   "metadata": {},
   "source": [
    "En dos terminales ejecutar:\n",
    "\n",
    "```bash\n",
    "docker exec -it kafka bash\n",
    "```\n",
    "y luego en uno esto:\n",
    "\n",
    "```bash\n",
    " ./opt/kafka/bin/kafka-console-consumer.sh \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic mydata_prediction_response\n",
    "```\n",
    "\n",
    "y en otro:\n",
    "```bash\n",
    "/opt/kafka/bin/kafka-console-producer.sh   --broker-list kafka:9092   --topic mydata_prediction_request   --property \"parse.key=false\"   --property \"key.separator=:\"   --property \"value.serializer=org.apache.kafka.common.serialization.StringSerializer\"\n",
    "```\n",
    "y de ejemplo en el producer:\n",
    "```bash\n",
    "{\"order_id\": 1, \"customer_id\": \"cust_123\", \"restaurant_id\": \"rest_456\", \"order_date_and_time\": \"2025-08-06T13:00:00\", \"delivery_date_and_time\": \"2025-08-06T13:45:00\", \"order_value\": 1000, \"delivery_fee\": 100, \"payment_method\": \"credit_card\", \"discounts_and_offers\": \"10% off\", \"commission_fee\": 100, \"payment_processing_fee\": 20, \"refunds/chargebacks\": 0}\n",
    "```\n",
    "\n",
    "Pudes abrir mongo express y ver el resultado http://localhost:8081/db/agile_data_science/mydata_prediction_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5d23a-8c70-4632-b5a6-a990986510b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, LongType\n",
    "from pyspark.sql.functions import from_json, col, hour, dayofweek, when\n",
    "from pyspark.ml import PipelineModel\n",
    "import pymongo\n",
    "\n",
    "# ---- SparkSession ----\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"script sc\")\n",
    "    .master(\"spark://agile:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"✅ SparkSession creada\")\n",
    "\n",
    "# ---- Modelo ----\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "print(\"✅ Modelo cargado\")\n",
    "\n",
    "# ---- Schema de entrada desde Kafka (con UUID y decimales) ----\n",
    "schema = StructType([\n",
    "    StructField(\"UUID\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"order_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"delivery_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"delivery_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"discounts_and_offers\", StringType(), True),\n",
    "    StructField(\"commission_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_processing_fee\", DoubleType(), True),\n",
    "    StructField(\"refunds/chargebacks\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# ---- Lectura desde Kafka ----\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"mydata_prediction_request\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_df = (\n",
    "    raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_data\")\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# ---- Enriquecido EXACTO como en entrenamiento ----\n",
    "df_enriched = (\n",
    "    json_df\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "    .withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"es_hora_punta\", when((col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1).otherwise(0))\n",
    "    .withColumn(\"has_discount\", when(col(\"discounts_and_offers\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"discount_value\", when(col(\"has_discount\") == 1, col(\"order_value\") * 0.1).otherwise(0.0))\n",
    "    .withColumn(\"refunded\", when(col(\"refunds/chargebacks\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# ---- Solo columnas que espera el pipeline + UUID + order_id ----\n",
    "required_numeric = [\n",
    "    \"order_value\", \"delivery_fee\", \"commission_fee\", \"payment_processing_fee\",\n",
    "    \"refunds/chargebacks\", \"discount_value\", \"has_discount\", \"refunded\",\n",
    "    \"day_of_week\", \"hour_of_day\", \"es_fin_de_semana\", \"es_hora_punta\"\n",
    "]\n",
    "required_categorical = [\"payment_method\", \"discounts_and_offers\"]\n",
    "required_cols = [\"UUID\", \"order_id\"] + required_numeric + required_categorical\n",
    "\n",
    "features_df = df_enriched.select(*required_cols)\n",
    "\n",
    "# ---- Predicción ----\n",
    "predicciones = modelo.transform(features_df)\n",
    "\n",
    "# Lo que vamos a persistir/publicar\n",
    "resultado = predicciones.select(\"UUID\", \"order_id\", \"prediction\")\n",
    "\n",
    "# ---- foreachBatch: escribe en Mongo y Kafka en el mismo microbatch ----\n",
    "def write_to_mongo_and_kafka(batch_df, epoch_id):\n",
    "    client = pymongo.MongoClient(\"mongo\")\n",
    "    db = client[\"agile_data_science\"]\n",
    "    out = db[\"mydata_prediction_response\"]\n",
    "    err = db[\"mydata_prediction_errors\"]\n",
    "    out.create_index(\"UUID\", unique=False)\n",
    "\n",
    "    try:\n",
    "        rows = [r.asDict() for r in batch_df.collect()]\n",
    "        if rows:\n",
    "            out.insert_many(rows)\n",
    "\n",
    "        (batch_df\n",
    "         .selectExpr(\"UUID as key\", \"to_json(struct(*)) as value\")\n",
    "         .write\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "         .option(\"topic\", \"mydata_prediction_response\")\n",
    "         .save())\n",
    "    except Exception as e:\n",
    "        err.insert_one({\n",
    "            \"epoch_id\": int(epoch_id),\n",
    "            \"error\": str(e),\n",
    "            \"sample_rows\": rows[:3] if 'rows' in locals() else [],\n",
    "        })\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "query = (resultado.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", \"/tmp/checkpoints-foreachbatch-v2\")  # cambia/borra si reinicias\n",
    "         .foreachBatch(write_to_mongo_and_kafka)\n",
    "         .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
