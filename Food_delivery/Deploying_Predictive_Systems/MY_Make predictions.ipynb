{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f854b8",
   "metadata": {},
   "source": [
    "Script en celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25191f",
   "metadata": {},
   "source": [
    "En dos terminales ejecutar:\n",
    "\n",
    "```bash\n",
    "docker exec -it kafka bash\n",
    "```\n",
    "y luego en uno esto:\n",
    "\n",
    "```bash\n",
    " ./opt/kafka/bin/kafka-console-consumer.sh \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic mydata_prediction_response\n",
    "```\n",
    "\n",
    "y en otro:\n",
    "```bash\n",
    "/opt/kafka/bin/kafka-console-producer.sh   --broker-list kafka:9092   --topic mydata_prediction_request   --property \"parse.key=false\"   --property \"key.separator=:\"   --property \"value.serializer=org.apache.kafka.common.serialization.StringSerializer\"\n",
    "```\n",
    "y de ejemplo en el producer:\n",
    "```bash\n",
    "{\"order_id\": 1, \"customer_id\": \"cust_123\", \"restaurant_id\": \"rest_456\", \"order_date_and_time\": \"2025-08-06T13:00:00\", \"delivery_date_and_time\": \"2025-08-06T13:45:00\", \"order_value\": 1000, \"delivery_fee\": 100, \"payment_method\": \"credit_card\", \"discounts_and_offers\": \"10% off\", \"commission_fee\": 100, \"payment_processing_fee\": 20, \"refunds/chargebacks\": 0}\n",
    "```\n",
    "\n",
    "Pudes abrir mongo express y ver el resultado http://localhost:8081/db/agile_data_science/mydata_prediction_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5d23a-8c70-4632-b5a6-a990986510b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-63d684ea-3e9a-4df0-aad0-fb4e907d3237;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1195ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-63d684ea-3e9a-4df0-aad0-fb4e907d3237\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/18ms)\n",
      "25/08/10 17:02:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession creada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo cargado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, LongType\n",
    "from pyspark.sql.functions import from_json, col, hour, dayofweek, when\n",
    "from pyspark.ml import PipelineModel\n",
    "import pymongo\n",
    "\n",
    "# ---- SparkSession ----\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"script sc\")\n",
    "    .master(\"spark://agile:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"✅ SparkSession creada\")\n",
    "\n",
    "# ---- Modelo ----\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "print(\"✅ Modelo cargado\")\n",
    "\n",
    "# ---- Schema de entrada desde Kafka (con UUID y decimales) ----\n",
    "schema = StructType([\n",
    "    StructField(\"UUID\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"order_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"delivery_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"delivery_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"discounts_and_offers\", StringType(), True),\n",
    "    StructField(\"commission_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_processing_fee\", DoubleType(), True),\n",
    "    StructField(\"refunds/chargebacks\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# ---- Lectura desde Kafka ----\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"mydata_prediction_request\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_df = (\n",
    "    raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_data\")\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# ---- Enriquecido EXACTO como en entrenamiento ----\n",
    "df_enriched = (\n",
    "    json_df\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "    .withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"es_hora_punta\", when((col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1).otherwise(0))\n",
    "    .withColumn(\"has_discount\", when(col(\"discounts_and_offers\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"discount_value\", when(col(\"has_discount\") == 1, col(\"order_value\") * 0.1).otherwise(0.0))\n",
    "    .withColumn(\"refunded\", when(col(\"refunds/chargebacks\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# ---- Solo columnas que espera el pipeline + UUID + order_id ----\n",
    "required_numeric = [\n",
    "    \"order_value\", \"delivery_fee\", \"commission_fee\", \"payment_processing_fee\",\n",
    "    \"refunds/chargebacks\", \"discount_value\", \"has_discount\", \"refunded\",\n",
    "    \"day_of_week\", \"hour_of_day\", \"es_fin_de_semana\", \"es_hora_punta\"\n",
    "]\n",
    "required_categorical = [\"payment_method\", \"discounts_and_offers\"]\n",
    "required_cols = [\"UUID\", \"order_id\"] + required_numeric + required_categorical\n",
    "\n",
    "features_df = df_enriched.select(*required_cols)\n",
    "\n",
    "# ---- Predicción ----\n",
    "predicciones = modelo.transform(features_df)\n",
    "\n",
    "# Lo que vamos a persistir/publicar\n",
    "resultado = predicciones.select(\"UUID\", \"order_id\", \"prediction\")\n",
    "\n",
    "# ---- foreachBatch: escribe en Mongo y Kafka en el mismo microbatch ----\n",
    "def write_to_mongo_and_kafka(batch_df, epoch_id):\n",
    "    client = pymongo.MongoClient(\"mongo\")\n",
    "    db = client[\"agile_data_science\"]\n",
    "    out = db[\"mydata_prediction_response\"]\n",
    "    err = db[\"mydata_prediction_errors\"]\n",
    "    out.create_index(\"UUID\", unique=False)\n",
    "\n",
    "    try:\n",
    "        rows = [r.asDict() for r in batch_df.collect()]\n",
    "        if rows:\n",
    "            out.insert_many(rows)\n",
    "\n",
    "        (batch_df\n",
    "         .selectExpr(\"UUID as key\", \"to_json(struct(*)) as value\")\n",
    "         .write\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "         .option(\"topic\", \"mydata_prediction_response\")\n",
    "         .save())\n",
    "    except Exception as e:\n",
    "        err.insert_one({\n",
    "            \"epoch_id\": int(epoch_id),\n",
    "            \"error\": str(e),\n",
    "            \"sample_rows\": rows[:3] if 'rows' in locals() else [],\n",
    "        })\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "query = (resultado.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", \"/tmp/checkpoints-foreachbatch-v2\")  # cambia/borra si reinicias\n",
    "         .foreachBatch(write_to_mongo_and_kafka)\n",
    "         .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
