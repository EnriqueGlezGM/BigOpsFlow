{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f854b8",
   "metadata": {
    "papermill": {
     "duration": 0.007175,
     "end_time": "2025-08-26T15:30:42.665121",
     "exception": false,
     "start_time": "2025-08-26T15:30:42.657946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Script en celda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25191f",
   "metadata": {
    "papermill": {
     "duration": 0.002849,
     "end_time": "2025-08-26T15:30:42.673716",
     "exception": false,
     "start_time": "2025-08-26T15:30:42.670867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "En dos terminales ejecutar:\n",
    "\n",
    "```bash\n",
    "docker exec -it kafka bash\n",
    "```\n",
    "y luego en uno esto:\n",
    "\n",
    "```bash\n",
    " ./opt/kafka/bin/kafka-console-consumer.sh \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic mydata_prediction_response\n",
    "```\n",
    "\n",
    "y en otro:\n",
    "```bash\n",
    "/opt/kafka/bin/kafka-console-producer.sh   --broker-list kafka:9092   --topic mydata_prediction_request   --property \"parse.key=false\"   --property \"key.separator=:\"   --property \"value.serializer=org.apache.kafka.common.serialization.StringSerializer\"\n",
    "```\n",
    "y de ejemplo en el producer:\n",
    "```bash\n",
    "{\"order_id\": 1, \"customer_id\": \"cust_123\", \"restaurant_id\": \"rest_456\", \"order_date_and_time\": \"2025-08-06T13:00:00\", \"delivery_date_and_time\": \"2025-08-06T13:45:00\", \"order_value\": 1000, \"delivery_fee\": 100, \"payment_method\": \"credit_card\", \"discounts_and_offers\": \"10% off\", \"commission_fee\": 100, \"payment_processing_fee\": 20, \"refunds/chargebacks\": 0}\n",
    "```\n",
    "\n",
    "Pudes abrir mongo express y ver el resultado http://localhost:8081/db/agile_data_science/mydata_prediction_response\n",
    "\n",
    "Para ejecutar el form:\n",
    "```bash\n",
    "docker exec -it agile bash\n",
    "``` \n",
    "y dentro de agile:\n",
    "```bash\n",
    "python /home/jovyan/Food_delivery/Deploying_Predictive_Systems/web/MY_flask_api.py\n",
    "``` \n",
    "\n",
    "Para ejecutar el notebook desde fuera:\n",
    "```bash\n",
    "papermill \"/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb\" \"/home/jovyan/Food_delivery/Deploying_Predictive_Systems/MY_Make predictions_out.ipynb\"\n",
    "\n",
    "docker exec -it -u jovyan agile bash -lc '\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PATH=\"$SPARK_HOME/bin:$PATH\"\n",
    "export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.2-src.zip:$PYTHONPATH\"\n",
    "\n",
    "papermill \"/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb\" \"/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions_out.ipynb\"\n",
    "'\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5d23a-8c70-4632-b5a6-a990986510b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T15:30:42.706490Z",
     "iopub.status.busy": "2025-08-26T15:30:42.693176Z",
     "iopub.status.idle": "2025-08-26T15:36:40.498685Z",
     "shell.execute_reply": "2025-08-26T15:36:40.496547Z"
    },
    "papermill": {
     "duration": 357.65047,
     "end_time": "2025-08-26T15:36:40.327793",
     "exception": false,
     "start_time": "2025-08-26T15:30:42.677323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CELDA √öNICA: Streaming Kafka -> Enriquecido -> Predicci√≥n -> Mongo + Kafka + Elasticsearch\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, LongType\n",
    "from pyspark.sql.functions import from_json, col, hour, dayofweek, when, current_timestamp\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "try:\n",
    "    # Si existe un SparkSession activo, ci√©rralo\n",
    "    _active = SparkSession.getActiveSession()\n",
    "    if _active is not None:\n",
    "        _active.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Si existe un SparkContext activo, det√©nlo\n",
    "    if SparkContext._active_spark_context is not None:\n",
    "        SparkContext._active_spark_context.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"üîß Creando SparkSession...\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"mydata-streaming-predict\")\n",
    "    .master(\"spark://agile:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"‚úÖ SparkSession creada\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Cargar modelo\n",
    "# -------------------------\n",
    "print(\"üîß Cargando PipelineModel...\")\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "print(\"‚úÖ Modelo cargado\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Esquema de entrada (incluye UUID)\n",
    "# -------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"UUID\", StringType(), True),\n",
    "    StructField(\"order_id\", LongType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"order_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"delivery_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"delivery_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"discounts_and_offers\", StringType(), True),\n",
    "    StructField(\"commission_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_processing_fee\", DoubleType(), True),\n",
    "    StructField(\"refunds/chargebacks\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# 3) Lectura desde Kafka\n",
    "# -------------------------\n",
    "print(\"üîå Conectando a Kafka (topic: mydata_prediction_request)...\")\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"mydata_prediction_request\")\n",
    "    .option(\"startingOffsets\", \"latest\")   # cambia a \"earliest\" si quieres re-consumir desde el inicio\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_df = (\n",
    "    raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json_data\")\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Enriquecido EXACTO como en el entrenamiento\n",
    "# -------------------------\n",
    "df_enriched = (\n",
    "    json_df\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "    .withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"es_hora_punta\", when(\n",
    "        (col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1\n",
    "    ).otherwise(0))\n",
    "    .withColumn(\"has_discount\", when(col(\"discounts_and_offers\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"discount_value\", when(col(\"has_discount\") == 1, col(\"order_value\") * 0.1).otherwise(0.0))\n",
    "    .withColumn(\"refunded\", when(col(\"refunds/chargebacks\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Selecci√≥n de columnas:\n",
    "#    - las que el pipeline necesita\n",
    "#    - + columnas de CONTEXTO que queremos conservar (pasan a trav√©s del modelo)\n",
    "# -------------------------\n",
    "required_numeric = [\n",
    "    \"order_value\", \"delivery_fee\", \"commission_fee\", \"payment_processing_fee\",\n",
    "    \"refunds/chargebacks\", \"discount_value\", \"has_discount\", \"refunded\",\n",
    "    \"day_of_week\", \"hour_of_day\", \"es_fin_de_semana\", \"es_hora_punta\"\n",
    "]\n",
    "required_categorical = [\"payment_method\", \"discounts_and_offers\"]\n",
    "context_cols = [\"UUID\", \"order_id\", \"order_date_and_time\", \"payment_method\", \"discounts_and_offers\"]\n",
    "features_df = df_enriched.select(*(required_numeric + required_categorical + context_cols))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Predicci√≥n\n",
    "# -------------------------\n",
    "predicciones = modelo.transform(features_df)\n",
    "\n",
    "# Documento final (sin joins)\n",
    "resultado_enriquecido = (\n",
    "    predicciones\n",
    "    .select(\n",
    "        \"UUID\",\n",
    "        \"order_id\",\n",
    "        \"prediction\",\n",
    "        \"order_date_and_time\",\n",
    "        \"payment_method\",\n",
    "        \"discounts_and_offers\",\n",
    "        \"day_of_week\",\n",
    "        \"hour_of_day\",\n",
    "        \"es_fin_de_semana\",\n",
    "        \"es_hora_punta\"\n",
    "    )\n",
    "    .withColumn(\"@ingest_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7) foreachBatch: Mongo + Kafka + Elasticsearch (Bulk)\n",
    "# -------------------------\n",
    "def write_to_mongo_kafka_es(batch_df, epoch_id):\n",
    "    import pymongo, json, urllib.request, urllib.error\n",
    "    from uuid import uuid4\n",
    "\n",
    "    client = None\n",
    "    try:\n",
    "        # Colectar microbatch (en dicts normales)\n",
    "        rows = [r.asDict() for r in batch_df.collect()]\n",
    "        print(f\"üß± Microbatch {epoch_id}: {len(rows)} docs\")\n",
    "\n",
    "        if not rows:\n",
    "            return\n",
    "\n",
    "        # --- Prepara versi√≥n SANEADA para ES (NUNCA con _id)\n",
    "        es_rows = []\n",
    "        for d in rows:\n",
    "            d_es = dict(d)        # copia\n",
    "            d_es.pop(\"_id\", None) # por si viniera de otra etapa\n",
    "            es_rows.append(d_es)\n",
    "\n",
    "        # --- Construye y env√≠a BULK NDJSON a ES ANTES de tocar Mongo\n",
    "        ndjson_lines = []\n",
    "        for d in es_rows:\n",
    "            es_id = d.get(\"UUID\") or str(uuid4())  # idempotencia\n",
    "            ndjson_lines.append(json.dumps({\"index\": {\"_index\": \"mydata_prediction_response\", \"_id\": es_id}}))\n",
    "            ndjson_lines.append(json.dumps(d, default=str))\n",
    "        payload = (\"\\n\".join(ndjson_lines) + \"\\n\").encode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            req = urllib.request.Request(\n",
    "                \"http://elastic:9200/_bulk\",\n",
    "                data=payload,\n",
    "                headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "                method=\"POST\",\n",
    "            )\n",
    "            with urllib.request.urlopen(req, timeout=15) as resp:\n",
    "                body = resp.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            # Parseamos y solo mostramos si errors=true\n",
    "            es_resp = json.loads(body)\n",
    "            if es_resp.get(\"errors\", False):\n",
    "                print(\"‚ùå ES bulk con errores:\", body[:800], \"...\")\n",
    "                \n",
    "        except urllib.error.HTTPError as he:\n",
    "            body = he.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            print(f\"‚ùå ES HTTPError {he.code}: {body[:800]}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error conectando a ES:\", e)\n",
    "\n",
    "        # --- Kafka (respuesta): usa el batch_df tal cual\n",
    "        (batch_df\n",
    "         .selectExpr(\"UUID as key\", \"to_json(struct(*)) as value\")\n",
    "         .write\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "         .option(\"topic\", \"mydata_prediction_response\")\n",
    "         .save())\n",
    "\n",
    "        # --- Mongo: inserta COPIAS para que PyMongo pueda inyectar _id sin contaminar nada\n",
    "        client = pymongo.MongoClient(\"mongo\")\n",
    "        db = client[\"agile_data_science\"]\n",
    "        out = db[\"mydata_prediction_response\"]\n",
    "        out.create_index(\"UUID\", unique=False)\n",
    "\n",
    "        rows_for_mongo = [dict(d) for d in es_rows]  # copia independiente\n",
    "        out.insert_many(rows_for_mongo)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en foreachBatch:\", e)\n",
    "        try:\n",
    "            if client is None:\n",
    "                client = pymongo.MongoClient(\"mongo\")\n",
    "            db = client[\"agile_data_science\"]\n",
    "            db[\"mydata_prediction_errors\"].insert_one({\n",
    "                \"epoch_id\": int(epoch_id),\n",
    "                \"error\": str(e),\n",
    "                \"note\": \"Fallo en foreachBatch\",\n",
    "            })\n",
    "        except Exception as e2:\n",
    "            print(\"‚ùå Error registrando error en Mongo:\", e2)\n",
    "    finally:\n",
    "        try:\n",
    "            client and client.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# -------------------------\n",
    "# 8) Lanzar el stream\n",
    "# -------------------------\n",
    "print(\"üöÄ Iniciando streaming (Mongo + Kafka + Elasticsearch)...\")\n",
    "query = (resultado_enriquecido.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", \"/tmp/checkpoints-foreachbatch-es-v6\")  # cambia si reinicias\n",
    "         .foreachBatch(write_to_mongo_kafka_es)\n",
    "         .start())\n",
    "\n",
    "print(\"‚è≥ Esperando microbatches...\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 359.449422,
   "end_time": "2025-08-26T15:36:40.765692",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb",
   "output_path": "/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb",
   "parameters": {},
   "start_time": "2025-08-26T15:30:41.316270",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
