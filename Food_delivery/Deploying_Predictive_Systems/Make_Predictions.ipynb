{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fba943",
   "metadata": {
    "papermill": {
     "duration": 0.004944,
     "end_time": "2025-08-29T16:19:04.177564",
     "exception": false,
     "start_time": "2025-08-29T16:19:04.172620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Celda de Streaming: Kafka ‚Üí Spark ‚Üí MongoDB + Kafka + Elasticsearch\n",
    "\n",
    "Esta celda pone en marcha **todo el pipeline de predicci√≥n en tiempo real**:\n",
    "\n",
    "1. **Crea una sesi√≥n de Spark** conectada al cl√∫ster definido en `docker-compose`.\n",
    "2. **Carga el modelo de Machine Learning** previamente entrenado y guardado en `./models/pipeline_model.bin`.\n",
    "3. **Escucha en Kafka** el topic `mydata_prediction_request`, donde llegan las peticiones de predicci√≥n.\n",
    "4. **Enriquece los datos de entrada** con variables derivadas (`d√≠a de la semana`, `hora punta`, `descuentos`, etc.).\n",
    "5. **Ejecuta el modelo de predicci√≥n** sobre los datos enriquecidos.\n",
    "6. **Persiste y publica los resultados**:\n",
    "   - En **MongoDB** ‚Üí colecci√≥n `mydata_prediction_response`.\n",
    "   - En **Kafka** ‚Üí topic `mydata_prediction_response`.\n",
    "   - En **Elasticsearch** ‚Üí √≠ndice `mydata_prediction_response` (para visualizarlos en Kibana).\n",
    "\n",
    "---\n",
    "\n",
    "### Qu√© esperar al ejecutarla\n",
    "- El notebook mostrar√° mensajes como `SparkSession creada`, `Modelo cargado`, y finalmente:  \n",
    "  ```\n",
    "  Iniciando streaming (Mongo + Kafka + Elasticsearch)...\n",
    "  Esperando microbatches...\n",
    "  ```\n",
    "- A partir de ese momento, la celda queda **en ejecuci√≥n continua**: cada vez que lleguen datos a Kafka, ser√°n procesados y enviados a Mongo, Kafka y Elasticsearch.\n",
    "- Ver√°s logs de microbatches indicando cu√°ntos documentos se han procesado.\n",
    "\n",
    "---\n",
    "\n",
    "### C√≥mo probarlo\n",
    "1. Lanza la API web (contenedor `predict_api`) y abre:  \n",
    "   [http://localhost:5050/](http://localhost:5050/)  \n",
    "2. Rellena el formulario y env√≠alo: eso genera un JSON con los datos y lo env√≠a al topic `mydata_prediction_request` en Kafka.\n",
    "3. Si la celda de streaming est√° activa, el microbatch recoger√° el evento, aplicar√° el modelo y guardar√° la predicci√≥n.\n",
    "4. Los resultados se pueden consultar en:\n",
    "   - **Mongo Express** ‚Üí [http://localhost:8081/db/agile_data_science/mydata_prediction_response](http://localhost:8081/db/agile_data_science/mydata_prediction_response)  \n",
    "   - **Kibana** ‚Üí [http://localhost:5601/](http://localhost:5601/) (dashboard con el √≠ndice `mydata_prediction_response`)\n",
    "\n",
    "---\n",
    "\n",
    "**Importante:**  \n",
    "Esta celda debe estar ejecut√°ndose en el notebook para que el flujo de predicciones funcione.  \n",
    "Si la detienes, no se procesar√°n nuevas peticiones hasta volver a lanzarla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5d23a-8c70-4632-b5a6-a990986510b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T16:19:04.214541Z",
     "iopub.status.busy": "2025-08-29T16:19:04.205443Z",
     "iopub.status.idle": "2025-08-28T18:21:35.584588Z",
     "shell.execute_reply": "2025-08-28T18:21:35.587599Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-08-29T16:19:04.181519",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# CELDA √öNICA: Streaming Kafka -> Enriquecido -> Predicci√≥n -> Mongo + Kafka + Elasticsearch\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType, LongType\n",
    "from pyspark.sql.functions import from_json, col, hour, dayofweek, when, current_timestamp\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "try:\n",
    "    # Si existe un SparkSession activo, ci√©rralo\n",
    "    _active = SparkSession.getActiveSession()\n",
    "    if _active is not None:\n",
    "        _active.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Si existe un SparkContext activo, det√©nlo\n",
    "    if SparkContext._active_spark_context is not None:\n",
    "        SparkContext._active_spark_context.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"üîß Creando SparkSession...\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"mydata-streaming-predict\")\n",
    "    .master(\"spark://agile:7077\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"‚úÖ SparkSession creada\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Cargar modelo\n",
    "# -------------------------\n",
    "print(\"üîß Cargando PipelineModel...\")\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "print(\"‚úÖ Modelo cargado\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Esquema de entrada (incluye UUID)\n",
    "# -------------------------\n",
    "schema = StructType([\n",
    "    StructField(\"UUID\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"restaurant_id\", StringType(), True),\n",
    "    StructField(\"order_date_and_time\", TimestampType(), True),\n",
    "    StructField(\"order_value\", DoubleType(), True),\n",
    "    StructField(\"delivery_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"discounts_and_offers\", StringType(), True),\n",
    "    StructField(\"commission_fee\", DoubleType(), True),\n",
    "    StructField(\"payment_processing_fee\", DoubleType(), True),\n",
    "    StructField(\"refunds/chargebacks\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# --- Crear los topics de Kafka si no existen ---\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "try:\n",
    "    admin = KafkaAdminClient(bootstrap_servers=\"kafka:9092\", client_id=\"init-topics\")\n",
    "    topics = [NewTopic(name=\"mydata_prediction_request\", num_partitions=1, replication_factor=1),\n",
    "              NewTopic(name=\"mydata_prediction_response\", num_partitions=1, replication_factor=1)]\n",
    "    admin.create_topics(topics)\n",
    "    print(\"‚úÖ Topics de Kafka creados\")\n",
    "    admin.close()\n",
    "except TopicAlreadyExistsError:\n",
    "    print(\"‚ÑπÔ∏è  Los topics ya exist√≠an\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  No se pudieron crear los topics autom√°ticamente: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Lectura desde Kafka\n",
    "# -------------------------\n",
    "print(\"üîå Conectando a Kafka (topic: mydata_prediction_request)...\")\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"mydata_prediction_request\")\n",
    "    .option(\"startingOffsets\", \"latest\")   # cambia a \"earliest\" si quieres re-consumir desde el inicio\n",
    "    .load()\n",
    ")\n",
    "\n",
    "json_df = (\n",
    "    raw_stream\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json_data\")\n",
    "    .select(from_json(\"json_data\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) Enriquecido EXACTO como en el entrenamiento\n",
    "# -------------------------\n",
    "df_enriched = (\n",
    "    json_df\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "    .withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "    .withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "    .withColumn(\"es_hora_punta\", when(\n",
    "        (col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1\n",
    "    ).otherwise(0))\n",
    "    .withColumn(\"has_discount\", when(col(\"discounts_and_offers\").isNotNull(), 1).otherwise(0))\n",
    "    .withColumn(\"discount_value\", when(col(\"has_discount\") == 1, col(\"order_value\") * 0.1).otherwise(0.0))\n",
    "    .withColumn(\"refunded\", when(col(\"refunds/chargebacks\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Selecci√≥n de columnas:\n",
    "#    - las que el pipeline necesita\n",
    "#    - + columnas de CONTEXTO que queremos conservar (pasan a trav√©s del modelo)\n",
    "# -------------------------\n",
    "required_numeric = [\n",
    "    \"order_value\", \"delivery_fee\", \"commission_fee\", \"payment_processing_fee\",\n",
    "    \"refunds/chargebacks\", \"discount_value\", \"has_discount\", \"refunded\",\n",
    "    \"day_of_week\", \"hour_of_day\", \"es_fin_de_semana\", \"es_hora_punta\"\n",
    "]\n",
    "required_categorical = [\"payment_method\", \"discounts_and_offers\"]\n",
    "context_cols = [\"UUID\", \"order_date_and_time\", \"payment_method\", \"discounts_and_offers\"]\n",
    "features_df = df_enriched.select(*(required_numeric + required_categorical + context_cols))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Predicci√≥n\n",
    "# -------------------------\n",
    "predicciones = modelo.transform(features_df)\n",
    "\n",
    "# Documento final (sin joins)\n",
    "resultado_enriquecido = (\n",
    "    predicciones\n",
    "    .select(\n",
    "        \"UUID\",\n",
    "        \"prediction\",\n",
    "        \"order_date_and_time\",\n",
    "        \"payment_method\",\n",
    "        \"discounts_and_offers\",\n",
    "        \"day_of_week\",\n",
    "        \"hour_of_day\",\n",
    "        \"es_fin_de_semana\",\n",
    "        \"es_hora_punta\"\n",
    "    )\n",
    "    .withColumn(\"@ingest_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 7) foreachBatch: Mongo + Kafka + Elasticsearch (Bulk)\n",
    "# -------------------------\n",
    "def write_to_mongo_kafka_es(batch_df, epoch_id):\n",
    "    import pymongo, json, urllib.request, urllib.error\n",
    "    from uuid import uuid4\n",
    "\n",
    "    client = None\n",
    "    try:\n",
    "        # Colectar microbatch (en dicts normales)\n",
    "        rows = [r.asDict() for r in batch_df.collect()]\n",
    "        print(f\"üß± Microbatch {epoch_id}: {len(rows)} docs\")\n",
    "\n",
    "        if not rows:\n",
    "            return\n",
    "\n",
    "        # --- Prepara versi√≥n SANEADA para ES (NUNCA con _id)\n",
    "        es_rows = []\n",
    "        for d in rows:\n",
    "            d_es = dict(d)        # copia\n",
    "            d_es.pop(\"_id\", None) # por si viniera de otra etapa\n",
    "            es_rows.append(d_es)\n",
    "\n",
    "        # --- Construye y env√≠a BULK NDJSON a ES ANTES de tocar Mongo\n",
    "        ndjson_lines = []\n",
    "        for d in es_rows:\n",
    "            es_id = d.get(\"UUID\") or str(uuid4())  # idempotencia\n",
    "            ndjson_lines.append(json.dumps({\"index\": {\"_index\": \"mydata_prediction_response\", \"_id\": es_id}}))\n",
    "            ndjson_lines.append(json.dumps(d, default=str))\n",
    "        payload = (\"\\n\".join(ndjson_lines) + \"\\n\").encode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            req = urllib.request.Request(\n",
    "                \"http://elastic:9200/_bulk\",\n",
    "                data=payload,\n",
    "                headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "                method=\"POST\",\n",
    "            )\n",
    "            with urllib.request.urlopen(req, timeout=15) as resp:\n",
    "                body = resp.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            # Parseamos y solo mostramos si errors=true\n",
    "            es_resp = json.loads(body)\n",
    "            if es_resp.get(\"errors\", False):\n",
    "                print(\"‚ùå ES bulk con errores:\", body[:800], \"...\")\n",
    "                \n",
    "        except urllib.error.HTTPError as he:\n",
    "            body = he.read().decode(\"utf-8\", errors=\"replace\")\n",
    "            print(f\"‚ùå ES HTTPError {he.code}: {body[:800]}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error conectando a ES:\", e)\n",
    "\n",
    "        # --- Kafka (respuesta): usa el batch_df tal cual\n",
    "        (batch_df\n",
    "         .selectExpr(\"UUID as key\", \"to_json(struct(*)) as value\")\n",
    "         .write\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "         .option(\"topic\", \"mydata_prediction_response\")\n",
    "         .save())\n",
    "\n",
    "        # --- Mongo: inserta COPIAS para que PyMongo pueda inyectar _id sin contaminar nada\n",
    "        client = pymongo.MongoClient(\"mongo\")\n",
    "        db = client[\"agile_data_science\"]\n",
    "        out = db[\"mydata_prediction_response\"]\n",
    "        out.create_index(\"UUID\", unique=False)\n",
    "\n",
    "        rows_for_mongo = [dict(d) for d in es_rows]  # copia independiente\n",
    "        out.insert_many(rows_for_mongo)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en foreachBatch:\", e)\n",
    "        try:\n",
    "            if client is None:\n",
    "                client = pymongo.MongoClient(\"mongo\")\n",
    "            db = client[\"agile_data_science\"]\n",
    "            db[\"mydata_prediction_errors\"].insert_one({\n",
    "                \"epoch_id\": int(epoch_id),\n",
    "                \"error\": str(e),\n",
    "                \"note\": \"Fallo en foreachBatch\",\n",
    "            })\n",
    "        except Exception as e2:\n",
    "            print(\"‚ùå Error registrando error en Mongo:\", e2)\n",
    "    finally:\n",
    "        try:\n",
    "            client and client.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# -------------------------\n",
    "# 8) Lanzar el stream\n",
    "# -------------------------\n",
    "print(\"üöÄ Iniciando streaming (Mongo + Kafka + Elasticsearch)...\")\n",
    "query = (resultado_enriquecido.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", \"/tmp/checkpoints-foreachbatch-es-v7\")  # cambia si reinicias\n",
    "         .foreachBatch(write_to_mongo_kafka_es)\n",
    "         .start())\n",
    "\n",
    "print(\"‚è≥ Esperando microbatches...\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb",
   "output_path": "/home/jovyan/Food_delivery/Deploying_Predictive_Systems/Make_Predictions.ipynb",
   "parameters": {},
   "start_time": "2025-08-29T16:19:02.553927",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
