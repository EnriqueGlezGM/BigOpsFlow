{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deploying Predictive Systems\n",
    "\n",
    "Este notebook es una prueba de concepto para verificar el funcionamiento del modelo predictivo generado durante el análisis exploratorio previo. Aquí no se realiza un despliegue en producción ni se integra con Kafka o sistemas de streaming: el objetivo es simplemente cargar el modelo ya entrenado y comprobar que genera predicciones razonables sobre el dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configuración del entorno Spark\n",
    "\n",
    "Creamos una nueva sesión de Spark apuntando al clúster especificado. Es importante cerrar cualquier contexto activo anterior para evitar conflictos. Este entorno nos permitirá procesar el DataFrame y generar predicciones con el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T16:27:31.921051Z",
     "start_time": "2025-07-04T16:27:21.971649Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Cerrar SparkContext anterior si está activo\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n",
    "\n",
    "# SparkSession apuntando al cluster\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DeployMyModel\")\n",
    "    .master(\"spark://agile:7077\")  # Asegúrate de que 'agile' es el nombre correcto del master en tu Docker Compose\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Verificación de la conexión con Spark\n",
    "\n",
    "Comprobamos que Spark está correctamente conectado al clúster y validamos la información básica de la sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version)\n",
    "print(spark.sparkContext.master)\n",
    "print(\"Aplicación:\", spark.sparkContext.appName)\n",
    "print(spark.sparkContext._jsc.sc().getExecutorMemoryStatus().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Carga del dataset de prueba\n",
    "\n",
    "En esta prueba usaremos un subconjunto del dataset original utilizado en el análisis exploratorio. Este paso intenta copiar el fichero a la ubicación deseada, aunque puede omitirse si ya se dispone del archivo en `./data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"/home/jovyan/.cache/kagglehub/datasets/romanniki/food-delivery-cost-and-profitability/versions/1/food_orders_new_delhi (1).csv\" ./data/food_orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lectura de datos\n",
    "\n",
    "Leemos el archivo `food_orders.csv` como un DataFrame de Spark, infiriendo automáticamente el esquema. Mostramos las primeras filas para validar su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./data/food_orders.csv\"\n",
    "df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Enriquecimiento del dataset\n",
    "\n",
    "Añadimos columnas calculadas a partir de la fecha y otros campos para generar variables que el modelo necesita: día de la semana, hora del pedido, si fue en fin de semana, si fue en hora punta, si tenía descuento, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, hour, when, col\n",
    "\n",
    "# Añadir columnas calculadas en Spark\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"Order Date and Time\"))  # 1=Sunday, ..., 7=Saturday\n",
    "df = df.withColumn(\"hour_of_day\", hour(\"Order Date and Time\"))\n",
    "\n",
    "# Es fin de semana (sábado=7 o domingo=1)\n",
    "df = df.withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# Es hora punta (13-15 o 20-22)\n",
    "df = df.withColumn(\n",
    "    \"es_hora_punta\",\n",
    "    when((col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Order ID\", \"order_id\") \\\n",
    "       .withColumnRenamed(\"Customer ID\", \"customer_id\") \\\n",
    "       .withColumnRenamed(\"Restaurant ID\", \"restaurant_id\") \\\n",
    "       .withColumnRenamed(\"Order Date and Time\", \"order_date_and_time\") \\\n",
    "       .withColumnRenamed(\"Delivery Date and Time\", \"delivery_date_and_time\") \\\n",
    "       .withColumnRenamed(\"Order Value\", \"order_value\") \\\n",
    "       .withColumnRenamed(\"Delivery Fee\", \"delivery_fee\") \\\n",
    "       .withColumnRenamed(\"Payment Method\", \"payment_method\") \\\n",
    "       .withColumnRenamed(\"Discounts and Offers\", \"discounts_and_offers\") \\\n",
    "       .withColumnRenamed(\"Commission Fee\", \"commission_fee\") \\\n",
    "       .withColumnRenamed(\"Payment Processing Fee\", \"payment_processing_fee\") \\\n",
    "       .withColumnRenamed(\"Refunds/Chargebacks\", \"refunds/chargebacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, hour, when, col\n",
    "\n",
    "# Día de la semana y hora\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(\"order_date_and_time\"))\n",
    "df = df.withColumn(\"hour_of_day\", hour(\"order_date_and_time\"))\n",
    "\n",
    "# Fin de semana\n",
    "df = df.withColumn(\"es_fin_de_semana\", when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0))\n",
    "\n",
    "# Hora punta\n",
    "df = df.withColumn(\"es_hora_punta\", when(\n",
    "    (col(\"hour_of_day\").between(13, 15)) | (col(\"hour_of_day\").between(20, 22)), 1\n",
    ").otherwise(0))\n",
    "\n",
    "# Tiene descuento\n",
    "df = df.withColumn(\"has_discount\", when(\n",
    "    col(\"discounts_and_offers\").isNotNull() & (col(\"discounts_and_offers\") != \"None\"), 1\n",
    ").otherwise(0))\n",
    "\n",
    "# Valor del descuento\n",
    "df = df.withColumn(\"discount_value\", col(\"order_value\") * col(\"has_discount\").cast(\"int\") * 0.1)\n",
    "\n",
    "df = df.withColumn(\"refunded\", (col(\"refunds/chargebacks\") > 0).cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Carga y aplicación del modelo\n",
    "\n",
    "Cargamos el pipeline de Spark MLlib previamente entrenado (incluye transformadores + modelo) y lo aplicamos al DataFrame enriquecido para obtener predicciones sobre la duración estimada del pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "modelo = PipelineModel.load(\"./models/pipeline_model.bin\")\n",
    "predicciones = modelo.transform(df)\n",
    "\n",
    "predicciones.select(\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predicciones generadas\n",
    "\n",
    "Mostramos algunas de las predicciones generadas por el modelo. En este caso, la variable objetivo es la duración estimada del pedido (en minutos, por ejemplo), y el modelo devuelve un valor numérico continuo para cada fila del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusión\n",
    "\n",
    "Este notebook permite validar que el modelo entrenado previamente funciona correctamente cuando se aplica a nuevos datos. Para escenarios reales, sería necesario integrarlo con pipelines de streaming, almacenamiento persistente y visualización de resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
